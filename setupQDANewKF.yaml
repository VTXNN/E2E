retrain : True
comet_project_name : Vertex_CNN_FH
comet_experiment_name : QDANN_pretrained
description : QDANN_Bit_inputs

trainable : "QDiffArgMax"
weight_features : ['normed_trk_pt','trk_MVA1','normed_trk_eta'] #['abs_trk_word_pT','rescaled_trk_word_MVAquality','abs_trk_word_eta'] #
track_features : ['normed_trk_pt','trk_MVA1','normed_trk_over_eta'] #['abs_trk_word_pT','rescaled_trk_word_MVAquality','abs_trk_word_eta']  #['bit_trk_pt','rescaled_bit_MVA1','rescaled_bit_trk_z0_res','bit_trk_eta'] #['normed_trk_pt','trk_MVA1','normed_trk_eta'] #
pretrained : True
bit_inputs : False
pretrain_DA : False


QuantisedModelName : Quantised_model
UnquantisedModelName : NewKF_Unquantised_model
PretrainedModelName : NewKF_Unquantised_model


eval_folder : Qplots
data_folder : /home/cebrown/Documents/Datasets/VertexDatasets/NewKFData

Nlatent : 0

starting_lr : 0.001
qtrain_starting_lr : 0.0001

epochs : 75
qtrain_epochs : 1

z0_loss_weight : 1
crossentropy_loss_weight : 1

qtrain_z0_loss_weight : 1
qtrain_crossentropy_loss_weight : 2

Huber_delta : 0.5

l1regloss : 1e-5
l2regloss : 1e-10
nweightnodes : 10
nweightlayers : 2
nassocnodes : 20
nassoclayers : 2

Final_Sparsity : 0.3
Begin_step : 10
End_step : 60

QConfig:
  weight_1: 
    kernel_quantizer: quantized_bits(5,3,alpha=1)
    bias_quantizer: quantized_bits(7,1,alpha=1)
    activation : quantized_relu(11,2)
  weight_2:
    kernel_quantizer : quantized_bits(4,1,alpha=1)
    bias_quantizer : quantized_bits(7,1,alpha=1)
    activation : quantized_relu(11,2)
  weight_final:
    kernel_quantizer : quantized_bits(4,2,alpha=1)
    bias_quantizer : quantized_bits(8,1,alpha=1)
    activation : quantized_relu(9,1)
  conv:
    kernel_quantizer : quantized_bits(5,2,alpha=1)
    activation : quantized_relu(7,1)
  association_0 :
    kernel_quantizer : quantized_bits(8,4,alpha=1)
    bias_quantizer : quantized_bits(6,1,alpha=1)
    activation : quantized_relu(13,3)
  association_1 : 
    kernel_quantizer : quantized_bits(6,3,alpha=1)
    bias_quantizer : quantized_bits(6,1,alpha=1)
    activation : quantized_relu(9,5)
        
