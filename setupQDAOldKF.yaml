retrain : True
comet_project_name : Vertex_CNN_FH
comet_experiment_name : QDANN_trkWord
description : QDANN_fixed_point

trainable : "QDiffArgMax"
weight_features : ['abs_trk_word_pT','rescaled_trk_word_MVAquality','abs_trk_word_eta'] #['abs_trk_word_pT','rescaled_trk_word_MVAquality','abs_trk_word_eta'] #
track_features : ['abs_trk_word_pT','rescaled_trk_word_MVAquality','trk_z0_res'] #['abs_trk_word_pT','rescaled_trk_word_MVAquality','abs_trk_word_eta']  #['bit_trk_pt','rescaled_bit_MVA1','rescaled_bit_trk_z0_res','bit_trk_eta'] #['normed_trk_pt','trk_MVA1','normed_trk_eta'] #
pretrained : True
bit_inputs : False
pretrain_DA : True

QuantisedModelName : Quantised_model
UnquantisedModelName : Unquantised_model
PretrainedModelName : NewKF_Unquantised_model

eval_folder : Qplots
data_folder : /home/cebrown/Documents/Datasets/VertexDatasets/OldKFGTTData

Nlatent : 0

starting_lr : 0.0001
qtrain_starting_lr : 0.00001

epochs : 75
qtrain_epochs : 50

z0_loss_weight : 1
crossentropy_loss_weight : 1
qtrain_z0_loss_weight : 1
qtrain_crossentropy_loss_weight : 1

Huber_delta : 0.5

l1regloss : 1e-3
l2regloss : 1e-10
nweightnodes : 10
nweightlayers : 2
nassocnodes : 20
nassoclayers : 2

Final_Sparsity : 0.7
Begin_step : 2000
End_step : 12000

QConfig:
  weight_1: 
    kernel_quantizer: quantized_bits(2,1,alpha=1)
    bias_quantizer: quantized_bits(4,1,alpha=1)
    activation : quantized_relu(9,1)
  weight_2:
    kernel_quantizer : quantized_bits(2,1,alpha=1)
    bias_quantizer : quantized_bits(4,1,alpha=1)
    activation : quantized_relu(9,1)
  weight_final:
    kernel_quantizer : quantized_bits(2,1,alpha=1)
    bias_quantizer : quantized_bits(2,1,alpha=1)
    activation : quantized_relu(9,1)
  conv:
    kernel_quantizer : quantized_bits(2,1,alpha=1)
    activation : quantized_relu(5,1)
  association_0 :
    kernel_quantizer : quantized_bits(4,2,alpha=1)
    bias_quantizer : quantized_bits(3,1,alpha=1)
    activation : quantized_relu(10,2)
  association_1 : 
    kernel_quantizer : quantized_bits(4,2,alpha=1)
    bias_quantizer : quantized_bits(3,1,alpha=1)
    activation : quantized_relu(6,2)
  association_final : 
    kernel_quantizer : quantized_bits(4,2,alpha=1)
    bias_quantizer : quantized_bits(3,1,alpha=1)
    
hls4ml_weight :
  model:
    quantizer : 20,10 
  input:
    quantizer : 20,10
  weight_1: 
    kernel_quantizer: 20,10
    bias_quantizer: 20,10
    activation : 20,10
  weight_2:
    kernel_quantizer : 20,10
    bias_quantizer : 20,10
    activation : 20,10
  weight_final:
    kernel_quantizer : 20,10
    bias_quantizer : 20,10
    activation : 20,10

hls4ml_conv :
  model:
      quantizer : 32,16
  input:
    quantizer : 32,16
  conv:
    kernel_quantizer : 32,16
    activation : 32,16

hls4ml_assoc :
  model:
    quantizer : 20,10 
  input:
    quantizer : 20,10
  assoc_1: 
    kernel_quantizer: 20,10
    bias_quantizer: 20,10
    activation : 20,10
  assoc_2:
    kernel_quantizer : 20,10
    bias_quantizer : 20,10
    activation : 20,10
  assoc_final:
    kernel_quantizer : 20,10
    bias_quantizer : 20,10


